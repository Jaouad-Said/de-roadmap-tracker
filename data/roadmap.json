{
  "phases": [
    {
      "id": "phase-1",
      "title": "Core Data Engineering Foundations",
      "duration": "2-3 months",
      "description": "Master the fundamental skills every data engineer needs. This phase builds your core toolkit for handling data at any scale.",
      "order": 1,
      "sections": [
        {
          "id": "sql-data-modeling",
          "title": "SQL & Data Modeling",
          "topics": [
            {
              "id": "topic-sql-1",
              "title": "Advanced SQL: Window Functions (ROW_NUMBER, RANK, LAG/LEAD, NTILE)",
              "completed": false
            },
            {
              "id": "topic-sql-2",
              "title": "Common Table Expressions (CTEs) and Recursive Queries",
              "completed": false
            },
            {
              "id": "topic-sql-3",
              "title": "Query Execution Plans and Performance Tuning",
              "completed": false
            },
            {
              "id": "topic-sql-4",
              "title": "Star Schema Design: Facts, Dimensions, and Grain",
              "completed": false
            },
            {
              "id": "topic-sql-5",
              "title": "Snowflake Schema and When to Use It",
              "completed": false
            },
            {
              "id": "topic-sql-6",
              "title": "Data Vault 2.0: Hubs, Links, and Satellites",
              "completed": false
            },
            {
              "id": "topic-sql-7",
              "title": "Normalization Forms (1NF to 3NF) vs Analytical Denormalization",
              "completed": false
            },
            {
              "id": "topic-sql-8",
              "title": "Index Types: B-Tree, Hash, Bitmap, and Covering Indexes",
              "completed": false
            },
            {
              "id": "topic-sql-9",
              "title": "Partitioning Strategies: Range, List, and Hash",
              "completed": false
            },
            {
              "id": "topic-sql-10",
              "title": "Slowly Changing Dimensions (SCD Type 1, 2, 3)",
              "completed": false
            },
            {
              "id": "topic-sql-11",
              "title": "PL/SQL and Stored Procedures for Data Processing",
              "completed": false
            },
            {
              "id": "topic-sql-12",
              "title": "Working with JSON/NoSQL Data in SQL (JSON functions, unstructured data)",
              "completed": false
            },
            {
              "id": "topic-sql-13",
              "title": "Enterprise DWH: Teradata and Hive SQL Dialects",
              "completed": false
            }
          ],
          "tasks": [
            {
              "id": "task-sql-1",
              "title": "Complete LeetCode SQL 50 problems (Medium/Hard)",
              "completed": false
            },
            {
              "id": "task-sql-2",
              "title": "Design star schema for e-commerce analytics (orders, products, customers, time)",
              "completed": false
            },
            {
              "id": "task-sql-3",
              "title": "Write 5 complex reporting queries with window functions",
              "completed": false
            },
            {
              "id": "task-sql-4",
              "title": "Optimize a slow query from >10s to <1s using EXPLAIN ANALYZE",
              "completed": false
            },
            {
              "id": "task-sql-5",
              "title": "Implement SCD Type 2 for a customer dimension table",
              "completed": false
            },
            {
              "id": "task-sql-6",
              "title": "Create PostgreSQL database with proper indexing strategy",
              "completed": false
            },
            {
              "id": "task-sql-7",
              "title": "Practice querying JSON data and building dimensional models from unstructured sources",
              "completed": false
            },
            {
              "id": "task-sql-8",
              "title": "Write stored procedures for ETL data validation and transformation",
              "completed": false
            }
          ],
          "attachments": [
            {
              "id": "attachment-cf377b8a",
              "type": "file",
              "title": "5_Fundamentals-of-Data-Engineering-by-Joe-Reis-and-Matt-Housley_bibis.ir.pdf",
              "fileType": "pdf",
              "createdAt": "2025-12-19T08:41:09.604Z"
            }
          ],
          "why": "This is your daily language. Every data engineer writes SQL constantly. Understanding data modeling separates good engineers from great ones - you'll design schemas that make analysts love you and queries that actually perform.",
          "how": "Build a small analytics database for a fictional e-commerce company. Create fact and dimension tables, write complex reporting queries, and optimize them. Use PostgreSQL or MySQL locally.",
          "order": 1
        },
        {
          "id": "python-programming",
          "title": "Python for Data Engineering",
          "topics": [
            {
              "id": "topic-python-1",
              "title": "Python Data Structures: Lists, Dicts, Sets, and Comprehensions",
              "completed": false
            },
            {
              "id": "topic-python-2",
              "title": "Object-Oriented Programming: Classes, Inheritance, and Design Patterns",
              "completed": false
            },
            {
              "id": "topic-python-3",
              "title": "Pandas: DataFrames, GroupBy, Merge, and Pivot Operations",
              "completed": false
            },
            {
              "id": "topic-python-4",
              "title": "NumPy: Arrays, Vectorization, and Broadcasting",
              "completed": false
            },
            {
              "id": "topic-python-5",
              "title": "File Formats: CSV, JSON, Parquet, and Avro",
              "completed": false
            },
            {
              "id": "topic-python-6",
              "title": "REST APIs: Requests, Authentication (OAuth, API Keys), Pagination",
              "completed": false
            },
            {
              "id": "topic-python-7",
              "title": "Error Handling: Try/Except, Custom Exceptions, Logging Module",
              "completed": false
            },
            {
              "id": "topic-python-8",
              "title": "Environment Management: venv, Poetry, pip-tools",
              "completed": false
            },
            {
              "id": "topic-python-9",
              "title": "Type Hints and Pydantic for Data Validation",
              "completed": false
            },
            {
              "id": "topic-python-10",
              "title": "Testing: pytest, fixtures, and mocking",
              "completed": false
            }
          ],
          "tasks": [
            {
              "id": "task-python-1",
              "title": "Build ETL pipeline pulling from GitHub API with pagination",
              "completed": false
            },
            {
              "id": "task-python-2",
              "title": "Process 1GB CSV file efficiently with Pandas chunking",
              "completed": false
            },
            {
              "id": "task-python-3",
              "title": "Convert between JSON, CSV, and Parquet formats",
              "completed": false
            },
            {
              "id": "task-python-4",
              "title": "Implement retry logic with exponential backoff for API calls",
              "completed": false
            },
            {
              "id": "task-python-5",
              "title": "Create data validation class using Pydantic",
              "completed": false
            },
            {
              "id": "task-python-6",
              "title": "Write unit tests for ETL functions with pytest",
              "completed": false
            },
            {
              "id": "task-python-7",
              "title": "Set up logging that writes to both file and console",
              "completed": false
            }
          ],
          "attachments": [],
          "why": "Python is the glue language of data engineering. You'll use it for ETL scripts, API integrations, data quality checks, and automation. It's everywhere in the data stack.",
          "how": "Build an ETL pipeline that pulls data from a public API (like GitHub or weather API), transforms it with Pandas, handles errors gracefully, and saves to multiple formats.",
          "order": 2
        },
        {
          "id": "linux-cli-git",
          "title": "Linux, CLI & Git",
          "topics": [
            {
              "id": "topic-linux-1",
              "title": "Essential Commands: ls, cd, cp, mv, rm, chmod, chown",
              "completed": false
            },
            {
              "id": "topic-linux-2",
              "title": "Text Processing: grep, sed, awk, cut, sort, uniq",
              "completed": false
            },
            {
              "id": "topic-linux-3",
              "title": "Process Management: ps, top, htop, kill, nohup, screen",
              "completed": false
            },
            {
              "id": "topic-linux-4",
              "title": "Bash Scripting: Variables, Loops, Conditionals, Functions",
              "completed": false
            },
            {
              "id": "topic-linux-5",
              "title": "Git Fundamentals: add, commit, push, pull, merge, rebase",
              "completed": false
            },
            {
              "id": "topic-linux-6",
              "title": "Git Workflow: Feature Branches, Pull Requests, Code Review",
              "completed": false
            },
            {
              "id": "topic-linux-7",
              "title": "SSH: Key Generation, Config Files, Tunneling",
              "completed": false
            },
            {
              "id": "topic-linux-8",
              "title": "Cron Jobs: Syntax, Logging, Error Handling",
              "completed": false
            },
            {
              "id": "topic-linux-9",
              "title": "Environment Variables and .env Files",
              "completed": false
            }
          ],
          "tasks": [
            {
              "id": "task-linux-1",
              "title": "Set up WSL2 or Linux VM with development tools",
              "completed": false
            },
            {
              "id": "task-linux-2",
              "title": "Write bash script to process and aggregate log files",
              "completed": false
            },
            {
              "id": "task-linux-3",
              "title": "Create cron job that runs ETL script daily at 2 AM",
              "completed": false
            },
            {
              "id": "task-linux-4",
              "title": "Set up SSH keys for GitHub and remote server access",
              "completed": false
            },
            {
              "id": "task-linux-5",
              "title": "Practice Git workflow: create branch, make PR, resolve conflict",
              "completed": false
            },
            {
              "id": "task-linux-6",
              "title": "Write script to monitor disk space and send alerts",
              "completed": false
            }
          ],
          "attachments": [],
          "why": "Most data infrastructure runs on Linux. You'll SSH into servers, debug pipelines via command line, and collaborate through Git. These skills make you efficient and self-sufficient.",
          "how": "Set up a Linux VM or use WSL. Write bash scripts that automate daily tasks. Create a Git workflow for your projects with proper branching strategy.",
          "order": 3
        },
        {
          "id": "cloud-fundamentals",
          "title": "Cloud Fundamentals (Azure/GCP/AWS)",
          "topics": [
            {
              "id": "topic-cloud-1",
              "title": "Azure Core: Resource Groups, Virtual Networks, Azure Active Directory",
              "completed": false
            },
            {
              "id": "topic-cloud-2",
              "title": "Azure Blob Storage: Containers, Access Tiers, Lifecycle Management",
              "completed": false
            },
            {
              "id": "topic-cloud-3",
              "title": "Azure Data Lake Storage Gen2: Hierarchical Namespace, ACLs",
              "completed": false
            },
            {
              "id": "topic-cloud-4",
              "title": "GCP Core: Projects, IAM, Cloud Storage, BigQuery Basics",
              "completed": false
            },
            {
              "id": "topic-cloud-5",
              "title": "AWS Core: EC2, VPC, S3, IAM Roles and Policies",
              "completed": false
            },
            {
              "id": "topic-cloud-6",
              "title": "S3 Deep Dive: Storage Classes, Lifecycle Policies, Versioning",
              "completed": false
            },
            {
              "id": "topic-cloud-7",
              "title": "Cloud CLI Tools: Azure CLI, gcloud, AWS CLI",
              "completed": false
            },
            {
              "id": "topic-cloud-8",
              "title": "Lambda/Azure Functions/Cloud Functions: Serverless Data Processing",
              "completed": false
            },
            {
              "id": "topic-cloud-9",
              "title": "Cost Management: Budgets, Cost Explorer, Reserved Instances",
              "completed": false
            },
            {
              "id": "topic-cloud-10",
              "title": "Monitoring: CloudWatch, Azure Monitor, Cloud Logging",
              "completed": false
            }
          ],
          "tasks": [
            {
              "id": "task-cloud-1",
              "title": "Create Azure free account and set up billing alerts",
              "completed": false
            },
            {
              "id": "task-cloud-2",
              "title": "Build data lake structure in Azure Blob/ADLS (raw/processed/curated layers)",
              "completed": false
            },
            {
              "id": "task-cloud-3",
              "title": "Create GCP project with BigQuery sandbox for analytics",
              "completed": false
            },
            {
              "id": "task-cloud-4",
              "title": "Deploy Azure Function triggered by blob upload",
              "completed": false
            },
            {
              "id": "task-cloud-5",
              "title": "Build AWS S3 data lake with proper IAM roles",
              "completed": false
            },
            {
              "id": "task-cloud-6",
              "title": "Query cloud storage data using serverless SQL (Athena/BigQuery)",
              "completed": false
            },
            {
              "id": "task-cloud-7",
              "title": "Create monitoring dashboard for cloud resources",
              "completed": false
            }
          ],
          "attachments": [],
          "why": "Modern data engineering lives in the cloud. Azure is particularly in-demand in the job market, alongside GCP and AWS. Understanding multi-cloud is a strong differentiator.",
          "how": "Start with Azure (high market demand). Get free tier, build a data lake with Blob Storage/ADLS, set up IAM properly, and deploy serverless functions that process files.",
          "order": 4
        }
      ]
    },
    {
      "id": "phase-2",
      "title": "Pipeline & Orchestration",
      "duration": "2-3 months",
      "description": "Learn to build production-ready data pipelines that run reliably at scale. This is where you become a 'real' data engineer.",
      "order": 2,
      "sections": [
        {
          "id": "azure-data-platform",
          "title": "Azure Data Platform",
          "topics": [
            {
              "id": "topic-azure-1",
              "title": "Azure Data Factory: Pipelines, Datasets, Linked Services, Integration Runtime",
              "completed": false
            },
            {
              "id": "topic-azure-2",
              "title": "ADF Activities: Copy, Data Flow, Mapping Data Flows, Custom Activities",
              "completed": false
            },
            {
              "id": "topic-azure-3",
              "title": "Azure Databricks: Workspace, Clusters, Notebooks, Unity Catalog",
              "completed": false
            },
            {
              "id": "topic-azure-4",
              "title": "Databricks Delta Lake: ACID Transactions, Time Travel, OPTIMIZE",
              "completed": false
            },
            {
              "id": "topic-azure-5",
              "title": "Azure Synapse Analytics: Dedicated/Serverless SQL Pools, Spark Pools",
              "completed": false
            },
            {
              "id": "topic-azure-6",
              "title": "Synapse Pipelines: Integration with ADF, Orchestration Patterns",
              "completed": false
            },
            {
              "id": "topic-azure-7",
              "title": "Azure SQL Database and Managed Instance for Data Engineering",
              "completed": false
            },
            {
              "id": "topic-azure-8",
              "title": "Azure DevOps: CI/CD for Data Pipelines, Git Integration",
              "completed": false
            },
            {
              "id": "topic-azure-9",
              "title": "Medallion Architecture on Azure: Bronze, Silver, Gold Layers",
              "completed": false
            },
            {
              "id": "topic-azure-10",
              "title": "Azure Purview/Microsoft Purview: Data Catalog and Governance",
              "completed": false
            }
          ],
          "tasks": [
            {
              "id": "task-azure-1",
              "title": "Create Azure Data Factory pipeline with parameterized datasets",
              "completed": false
            },
            {
              "id": "task-azure-2",
              "title": "Build ETL pipeline: Blob Storage → Data Factory → SQL Database",
              "completed": false
            },
            {
              "id": "task-azure-3",
              "title": "Set up Azure Databricks workspace and run PySpark notebook",
              "completed": false
            },
            {
              "id": "task-azure-4",
              "title": "Implement medallion architecture in Databricks with Delta Lake",
              "completed": false
            },
            {
              "id": "task-azure-5",
              "title": "Create Synapse workspace and query data lake with serverless SQL",
              "completed": false
            },
            {
              "id": "task-azure-6",
              "title": "Set up CI/CD pipeline for ADF using Azure DevOps",
              "completed": false
            },
            {
              "id": "task-azure-7",
              "title": "Build end-to-end pipeline: API → ADLS → Databricks → Synapse → Power BI",
              "completed": false
            }
          ],
          "attachments": [],
          "why": "Azure is extremely prominent in the job market. Azure Data Factory, Databricks, and Synapse are mentioned in nearly every job posting. This is a must-have skill set.",
          "how": "Get Azure free account. Build a complete data pipeline using ADF for orchestration, Databricks for transformation, and Synapse for serving analytics. Connect to Power BI.",
          "order": 1
        },
        {
          "id": "workflow-orchestration",
          "title": "Workflow Orchestration (Airflow & dbt)",
          "topics": [
            {
              "id": "topic-orch-1",
              "title": "Airflow Architecture: Scheduler, Webserver, Workers, Metadata DB",
              "completed": false
            },
            {
              "id": "topic-orch-2",
              "title": "DAG Design: Dependencies, Triggers, Catchup, Backfill",
              "completed": false
            },
            {
              "id": "topic-orch-3",
              "title": "Operators: BashOperator, PythonOperator, Custom Operators",
              "completed": false
            },
            {
              "id": "topic-orch-4",
              "title": "Hooks and Connections: Database, S3, HTTP",
              "completed": false
            },
            {
              "id": "topic-orch-5",
              "title": "Sensors: FileSensor, ExternalTaskSensor, Custom Sensors",
              "completed": false
            },
            {
              "id": "topic-orch-6",
              "title": "TaskGroups and Dynamic DAG Generation",
              "completed": false
            },
            {
              "id": "topic-orch-7",
              "title": "XComs for Task Communication",
              "completed": false
            },
            {
              "id": "topic-orch-8",
              "title": "Airflow Best Practices: Idempotency, Atomicity, Incremental Loads",
              "completed": false
            },
            {
              "id": "topic-orch-9",
              "title": "dbt Core: Models, Sources, Refs, Macros, Jinja Templating",
              "completed": false
            },
            {
              "id": "topic-orch-10",
              "title": "dbt Testing: Schema Tests, Custom Tests, Data Freshness",
              "completed": false
            },
            {
              "id": "topic-orch-11",
              "title": "dbt Documentation: YAML, Docs Blocks, Lineage Graphs",
              "completed": false
            },
            {
              "id": "topic-orch-12",
              "title": "Integrating Airflow + dbt: Astronomer Cosmos, dbt Cloud API",
              "completed": false
            },
            {
              "id": "topic-orch-13",
              "title": "Cloud-Managed Orchestration: GCP Composer, AWS MWAA",
              "completed": false
            },
            {
              "id": "topic-orch-14",
              "title": "Modern Alternatives: Prefect, Dagster, Mage Comparison",
              "completed": false
            }
          ],
          "tasks": [
            {
              "id": "task-orch-1",
              "title": "Set up Airflow locally using Docker Compose",
              "completed": false
            },
            {
              "id": "task-orch-2",
              "title": "Create DAG with 5+ tasks and proper dependencies",
              "completed": false
            },
            {
              "id": "task-orch-3",
              "title": "Implement retry logic with exponential backoff",
              "completed": false
            },
            {
              "id": "task-orch-4",
              "title": "Build custom operator for API data extraction",
              "completed": false
            },
            {
              "id": "task-orch-5",
              "title": "Set up email alerts for task failures",
              "completed": false
            },
            {
              "id": "task-orch-6",
              "title": "Install dbt-core and initialize project connected to Snowflake/BigQuery",
              "completed": false
            },
            {
              "id": "task-orch-7",
              "title": "Build staging and mart models with proper dbt testing",
              "completed": false
            },
            {
              "id": "task-orch-8",
              "title": "Integrate dbt runs into Airflow DAG",
              "completed": false
            },
            {
              "id": "task-orch-9",
              "title": "Generate and publish dbt documentation site",
              "completed": false
            }
          ],
          "attachments": [],
          "why": "Airflow and dbt are the industry standard combo appearing in almost every job description. Mastering both together is essential for modern data engineering.",
          "how": "Install Airflow locally with Docker. Build a DAG that orchestrates your ETL pipeline. Add dbt for transformations with proper testing and documentation.",
          "order": 2
        },
        {
          "id": "batch-processing",
          "title": "Big Data Processing (Spark/Hadoop)",
          "topics": [
            {
              "id": "topic-batch-1",
              "title": "Spark Architecture: Driver, Executors, Cluster Managers",
              "completed": false
            },
            {
              "id": "topic-batch-2",
              "title": "RDDs vs DataFrames vs Datasets",
              "completed": false
            },
            {
              "id": "topic-batch-3",
              "title": "PySpark Transformations: map, filter, groupBy, join, window",
              "completed": false
            },
            {
              "id": "topic-batch-4",
              "title": "Spark SQL: SQL on DataFrames, Temp Views, UDFs",
              "completed": false
            },
            {
              "id": "topic-batch-5",
              "title": "Partitioning: repartition vs coalesce, partition pruning",
              "completed": false
            },
            {
              "id": "topic-batch-6",
              "title": "Optimization: Broadcast Joins, Caching, Spark UI Analysis",
              "completed": false
            },
            {
              "id": "topic-batch-7",
              "title": "Spark on Cloud: EMR, Dataproc, Azure Databricks",
              "completed": false
            },
            {
              "id": "topic-batch-8",
              "title": "Delta Lake: ACID Transactions, Time Travel, Z-Ordering, OPTIMIZE",
              "completed": false
            },
            {
              "id": "topic-batch-9",
              "title": "Apache Iceberg & Hudi: Open Table Formats, Schema Evolution",
              "completed": false
            },
            {
              "id": "topic-batch-10",
              "title": "Scala for Spark: Basics, Case Classes, Collections, Type Safety",
              "completed": false
            },
            {
              "id": "topic-batch-11",
              "title": "Hadoop Ecosystem: HDFS, YARN, Hive, MapReduce Concepts",
              "completed": false
            },
            {
              "id": "topic-batch-12",
              "title": "File Formats: Parquet, ORC, Avro, Delta - When to Use Each",
              "completed": false
            }
          ],
          "tasks": [
            {
              "id": "task-batch-1",
              "title": "Install PySpark locally and run first DataFrame job",
              "completed": false
            },
            {
              "id": "task-batch-2",
              "title": "Process NYC Taxi dataset (100M+ rows) with PySpark",
              "completed": false
            },
            {
              "id": "task-batch-3",
              "title": "Write complex aggregation with window functions in Spark",
              "completed": false
            },
            {
              "id": "task-batch-4",
              "title": "Optimize slow Spark job using Spark UI insights",
              "completed": false
            },
            {
              "id": "task-batch-5",
              "title": "Implement partitioned write to Delta Lake with Z-Ordering",
              "completed": false
            },
            {
              "id": "task-batch-6",
              "title": "Set up Spark job on Azure Databricks or EMR",
              "completed": false
            },
            {
              "id": "task-batch-7",
              "title": "Learn Scala basics and rewrite a PySpark job in Scala",
              "completed": false
            },
            {
              "id": "task-batch-8",
              "title": "Query Hive tables and understand Hadoop file system concepts",
              "completed": false
            }
          ],
          "attachments": [],
          "why": "PySpark is the #1 in-demand skill for data engineers. Many job offers also mention Scala, Hadoop, and Hive. Understanding the full big data ecosystem is crucial.",
          "how": "Set up Spark locally or on Databricks. Process large datasets with PySpark. Learn enough Scala to read/write Spark code. Understand Hive and Hadoop concepts.",
          "order": 3
        },
        {
          "id": "data-warehousing",
          "title": "Cloud Data Warehousing (Snowflake Focus)",
          "topics": [
            {
              "id": "topic-dw-1",
              "title": "Modern DW Architecture: Separation of Storage and Compute",
              "completed": false
            },
            {
              "id": "topic-dw-2",
              "title": "Snowflake Architecture: Virtual Warehouses, Storage, Cloud Services",
              "completed": false
            },
            {
              "id": "topic-dw-3",
              "title": "Snowflake Features: Time Travel, Zero-Copy Cloning, Data Sharing",
              "completed": false
            },
            {
              "id": "topic-dw-4",
              "title": "Snowflake Performance: Clustering, Materialized Views, Query Optimization",
              "completed": false
            },
            {
              "id": "topic-dw-5",
              "title": "Snowflake Data Loading: COPY INTO, Snowpipe, External Tables",
              "completed": false
            },
            {
              "id": "topic-dw-6",
              "title": "BigQuery: Slots, Partitioning, Clustering, Materialized Views",
              "completed": false
            },
            {
              "id": "topic-dw-7",
              "title": "Azure Synapse: Dedicated/Serverless SQL Pools, Distribution",
              "completed": false
            },
            {
              "id": "topic-dw-8",
              "title": "Redshift: Node Types, Distribution Styles, Sort Keys",
              "completed": false
            },
            {
              "id": "topic-dw-9",
              "title": "ELT vs ETL: Modern Transformation Patterns",
              "completed": false
            },
            {
              "id": "topic-dw-10",
              "title": "Slowly Changing Dimensions: Implementation Patterns in Cloud DW",
              "completed": false
            },
            {
              "id": "topic-dw-11",
              "title": "Data Lakehouse: Combining Lake Flexibility with Warehouse Features",
              "completed": false
            }
          ],
          "tasks": [
            {
              "id": "task-dw-1",
              "title": "Create free Snowflake trial account and explore the UI",
              "completed": false
            },
            {
              "id": "task-dw-2",
              "title": "Load sample data using COPY INTO from cloud storage",
              "completed": false
            },
            {
              "id": "task-dw-3",
              "title": "Set up Snowpipe for automated data ingestion",
              "completed": false
            },
            {
              "id": "task-dw-4",
              "title": "Create raw/staging/mart layers in Snowflake",
              "completed": false
            },
            {
              "id": "task-dw-5",
              "title": "Implement Time Travel queries and Zero-Copy Cloning",
              "completed": false
            },
            {
              "id": "task-dw-6",
              "title": "Optimize slow queries using clustering and materialized views",
              "completed": false
            },
            {
              "id": "task-dw-7",
              "title": "Connect dbt to Snowflake and build transformation models",
              "completed": false
            },
            {
              "id": "task-dw-8",
              "title": "Compare Snowflake vs BigQuery for a sample workload",
              "completed": false
            }
          ],
          "attachments": [],
          "why": "Snowflake is the most mentioned cloud data warehouse in job postings. Understanding its unique features plus alternatives like BigQuery and Synapse makes you versatile.",
          "how": "Get a free Snowflake trial. Load data, explore Time Travel and cloning. Build a complete analytics pipeline with dbt transformations.",
          "order": 4
        },
        {
          "id": "data-quality",
          "title": "Data Quality & Testing",
          "topics": [
            {
              "id": "topic-dq-1",
              "title": "Data Quality Dimensions: Accuracy, Completeness, Consistency, Timeliness",
              "completed": false
            },
            {
              "id": "topic-dq-2",
              "title": "Great Expectations: Expectations, Checkpoints, Data Docs",
              "completed": false
            },
            {
              "id": "topic-dq-3",
              "title": "Soda Core: YAML-based Data Quality Checks",
              "completed": false
            },
            {
              "id": "topic-dq-4",
              "title": "dbt Tests: Built-in, Custom, and Package Tests",
              "completed": false
            },
            {
              "id": "topic-dq-5",
              "title": "Data Contracts: Schema Definitions and SLAs",
              "completed": false
            },
            {
              "id": "topic-dq-6",
              "title": "Pipeline Monitoring: Metrics, Alerts, Dashboards",
              "completed": false
            },
            {
              "id": "topic-dq-7",
              "title": "Anomaly Detection: Statistical Methods for Data Drift",
              "completed": false
            },
            {
              "id": "topic-dq-8",
              "title": "Data Profiling: Understanding Data Distributions",
              "completed": false
            }
          ],
          "tasks": [
            {
              "id": "task-dq-1",
              "title": "Install Great Expectations and create first expectation suite",
              "completed": false
            },
            {
              "id": "task-dq-2",
              "title": "Define 10+ expectations for a production dataset",
              "completed": false
            },
            {
              "id": "task-dq-3",
              "title": "Create checkpoint that validates data on each pipeline run",
              "completed": false
            },
            {
              "id": "task-dq-4",
              "title": "Generate Data Docs and host as static site",
              "completed": false
            },
            {
              "id": "task-dq-5",
              "title": "Write custom expectation for business rule validation",
              "completed": false
            },
            {
              "id": "task-dq-6",
              "title": "Integrate Great Expectations into Airflow DAG",
              "completed": false
            },
            {
              "id": "task-dq-7",
              "title": "Set up alerts for data quality failures",
              "completed": false
            }
          ],
          "attachments": [],
          "why": "Bad data costs companies millions. Data quality is increasingly important and is a key differentiator for senior engineers. Quality-first mindset will set you apart.",
          "how": "Add Great Expectations to your pipeline. Define expectations for your data, build a validation suite, and integrate it into your Airflow DAG.",
          "order": 4
        }
      ]
    },
    {
      "id": "phase-3",
      "title": "Streaming & Real-time",
      "duration": "1-2 months",
      "description": "Add real-time capabilities to your skill set. Many companies need both batch and streaming, making this a valuable differentiator.",
      "order": 3,
      "sections": [
        {
          "id": "kafka-fundamentals",
          "title": "Apache Kafka Fundamentals",
          "topics": [
            {
              "id": "topic-kafka-1",
              "title": "Event Streaming Concepts: Events, Streams, Event-Driven Architecture",
              "completed": false
            },
            {
              "id": "topic-kafka-2",
              "title": "Kafka Architecture: Brokers, ZooKeeper/KRaft, Controller",
              "completed": false
            },
            {
              "id": "topic-kafka-3",
              "title": "Topics: Partitions, Replication Factor, Retention",
              "completed": false
            },
            {
              "id": "topic-kafka-4",
              "title": "Producers: Partitioning, Acknowledgments, Idempotence",
              "completed": false
            },
            {
              "id": "topic-kafka-5",
              "title": "Consumers: Consumer Groups, Offsets, Rebalancing",
              "completed": false
            },
            {
              "id": "topic-kafka-6",
              "title": "Kafka Connect: Source and Sink Connectors",
              "completed": false
            },
            {
              "id": "topic-kafka-7",
              "title": "Schema Registry: Avro, JSON Schema, Protobuf",
              "completed": false
            },
            {
              "id": "topic-kafka-8",
              "title": "Kafka Streams: Stateless and Stateful Operations",
              "completed": false
            },
            {
              "id": "topic-kafka-9",
              "title": "Kafka Security: SSL, SASL, ACLs",
              "completed": false
            }
          ],
          "tasks": [
            {
              "id": "task-kafka-1",
              "title": "Set up Kafka cluster locally with Docker Compose",
              "completed": false
            },
            {
              "id": "task-kafka-2",
              "title": "Create topic with multiple partitions and test replication",
              "completed": false
            },
            {
              "id": "task-kafka-3",
              "title": "Build Python producer that sends JSON events",
              "completed": false
            },
            {
              "id": "task-kafka-4",
              "title": "Build consumer with proper offset management",
              "completed": false
            },
            {
              "id": "task-kafka-5",
              "title": "Set up Schema Registry and use Avro serialization",
              "completed": false
            },
            {
              "id": "task-kafka-6",
              "title": "Configure Kafka Connect to sync from PostgreSQL to Kafka",
              "completed": false
            },
            {
              "id": "task-kafka-7",
              "title": "Build simple Kafka Streams application for aggregation",
              "completed": false
            }
          ],
          "attachments": [],
          "why": "Kafka is the backbone of real-time data architectures. Understanding event streaming opens up a whole new world of use cases - real-time analytics, event-driven systems, CDC.",
          "how": "Run Kafka locally with Docker. Build a producer that generates events, consume them with Python, and build a simple streaming pipeline.",
          "order": 1
        },
        {
          "id": "stream-processing",
          "title": "Stream Processing",
          "topics": [
            {
              "id": "topic-stream-1",
              "title": "Stream Processing Fundamentals: Unbounded Data, Event Time vs Processing Time",
              "completed": false
            },
            {
              "id": "topic-stream-2",
              "title": "Apache Flink: DataStream API, Table API, State Management",
              "completed": false
            },
            {
              "id": "topic-stream-3",
              "title": "Spark Structured Streaming: Micro-batch, Continuous Processing",
              "completed": false
            },
            {
              "id": "topic-stream-4",
              "title": "Windowing: Tumbling, Sliding, Session Windows",
              "completed": false
            },
            {
              "id": "topic-stream-5",
              "title": "Watermarks and Late Data Handling",
              "completed": false
            },
            {
              "id": "topic-stream-6",
              "title": "Exactly-Once Semantics: Checkpointing, Transactions",
              "completed": false
            },
            {
              "id": "topic-stream-7",
              "title": "State Management: RocksDB, Checkpoints, Savepoints",
              "completed": false
            },
            {
              "id": "topic-stream-8",
              "title": "Stream-Table Duality: KSQL, Flink SQL",
              "completed": false
            }
          ],
          "tasks": [
            {
              "id": "task-stream-1",
              "title": "Build Spark Structured Streaming app reading from Kafka",
              "completed": false
            },
            {
              "id": "task-stream-2",
              "title": "Implement tumbling window aggregation (events per minute)",
              "completed": false
            },
            {
              "id": "task-stream-3",
              "title": "Handle late data with watermarks",
              "completed": false
            },
            {
              "id": "task-stream-4",
              "title": "Write streaming data to Delta Lake with checkpointing",
              "completed": false
            },
            {
              "id": "task-stream-5",
              "title": "Set up Flink locally and run first streaming job",
              "completed": false
            },
            {
              "id": "task-stream-6",
              "title": "Build real-time dashboard with streaming aggregations",
              "completed": false
            }
          ],
          "attachments": [],
          "why": "Stream processing lets you react to data in real-time. From fraud detection to live dashboards, these skills command premium salaries.",
          "how": "Build a real-time analytics dashboard. Use Kafka for ingestion and Spark Streaming or Flink for processing. Aggregate metrics in tumbling windows.",
          "order": 2
        },
        {
          "id": "cdc-patterns",
          "title": "Change Data Capture (CDC)",
          "topics": [
            {
              "id": "topic-cdc-1",
              "title": "CDC Concepts: Log-based vs Query-based vs Trigger-based",
              "completed": false
            },
            {
              "id": "topic-cdc-2",
              "title": "Debezium Architecture: Connectors, Kafka Connect, Transforms",
              "completed": false
            },
            {
              "id": "topic-cdc-3",
              "title": "PostgreSQL CDC: Logical Replication, pgoutput Plugin",
              "completed": false
            },
            {
              "id": "topic-cdc-4",
              "title": "MySQL CDC: Binlog, GTID, Row-based Replication",
              "completed": false
            },
            {
              "id": "topic-cdc-5",
              "title": "CDC Event Formats: Before/After Images, Operations",
              "completed": false
            },
            {
              "id": "topic-cdc-6",
              "title": "Schema Evolution: Handling DDL Changes",
              "completed": false
            },
            {
              "id": "topic-cdc-7",
              "title": "CDC to Data Warehouse: Merge Patterns, Upserts",
              "completed": false
            },
            {
              "id": "topic-cdc-8",
              "title": "CDC Best Practices: Initial Snapshots, Monitoring, Recovery",
              "completed": false
            }
          ],
          "tasks": [
            {
              "id": "task-cdc-1",
              "title": "Set up Debezium with PostgreSQL connector",
              "completed": false
            },
            {
              "id": "task-cdc-2",
              "title": "Capture INSERT/UPDATE/DELETE events and send to Kafka",
              "completed": false
            },
            {
              "id": "task-cdc-3",
              "title": "Build consumer that applies CDC events to target database",
              "completed": false
            },
            {
              "id": "task-cdc-4",
              "title": "Implement upsert logic for data warehouse loading",
              "completed": false
            },
            {
              "id": "task-cdc-5",
              "title": "Handle schema changes gracefully in CDC pipeline",
              "completed": false
            },
            {
              "id": "task-cdc-6",
              "title": "Set up monitoring for CDC lag and throughput",
              "completed": false
            }
          ],
          "attachments": [],
          "why": "CDC enables real-time data replication without impacting source systems. It's how modern companies keep data warehouses in sync with production databases.",
          "how": "Set up Debezium to capture changes from a PostgreSQL database. Stream changes through Kafka to a target database or data warehouse.",
          "order": 3
        }
      ]
    },
    {
      "id": "phase-4",
      "title": "BI, Visualization & Analytics",
      "duration": "1-2 months",
      "description": "Master Business Intelligence tools and data visualization. Power BI is extremely in-demand. Learn to transform data into actionable insights and compelling dashboards.",
      "order": 4,
      "sections": [
        {
          "id": "power-bi",
          "title": "Power BI & Data Visualization",
          "topics": [
            {
              "id": "topic-pbi-1",
              "title": "Power BI Architecture: Desktop, Service, Gateway, Dataflows",
              "completed": false
            },
            {
              "id": "topic-pbi-2",
              "title": "Data Modeling: Star Schema, Relationships, Cardinality",
              "completed": false
            },
            {
              "id": "topic-pbi-3",
              "title": "DAX Fundamentals: CALCULATE, FILTER, ALL, Context Transition",
              "completed": false
            },
            {
              "id": "topic-pbi-4",
              "title": "Advanced DAX: Time Intelligence, Variables, Iterator Functions",
              "completed": false
            },
            {
              "id": "topic-pbi-5",
              "title": "Power Query (M Language): ETL, Data Transformation, Custom Functions",
              "completed": false
            },
            {
              "id": "topic-pbi-6",
              "title": "Visualization Best Practices: Chart Selection, Storytelling with Data",
              "completed": false
            },
            {
              "id": "topic-pbi-7",
              "title": "Report Design: Filters, Slicers, Drill-through, Bookmarks",
              "completed": false
            },
            {
              "id": "topic-pbi-8",
              "title": "Performance Optimization: Query Folding, Aggregations, Composite Models",
              "completed": false
            },
            {
              "id": "topic-pbi-9",
              "title": "Power BI Service: Workspaces, Apps, Row-Level Security (RLS)",
              "completed": false
            },
            {
              "id": "topic-pbi-10",
              "title": "Integration: DirectQuery, Import, Live Connection to Azure/Snowflake",
              "completed": false
            },
            {
              "id": "topic-pbi-11",
              "title": "Alternative Tools: Qlik Sense, Looker, Tableau Comparison",
              "completed": false
            }
          ],
          "tasks": [
            {
              "id": "task-pbi-1",
              "title": "Install Power BI Desktop and connect to sample data",
              "completed": false
            },
            {
              "id": "task-pbi-2",
              "title": "Build star schema data model with proper relationships",
              "completed": false
            },
            {
              "id": "task-pbi-3",
              "title": "Create calculated columns and measures using DAX",
              "completed": false
            },
            {
              "id": "task-pbi-4",
              "title": "Build time intelligence measures (YTD, YoY, Rolling Average)",
              "completed": false
            },
            {
              "id": "task-pbi-5",
              "title": "Design interactive dashboard with drill-through and bookmarks",
              "completed": false
            },
            {
              "id": "task-pbi-6",
              "title": "Connect Power BI to Snowflake/Azure SQL and build live report",
              "completed": false
            },
            {
              "id": "task-pbi-7",
              "title": "Publish report to Power BI Service and set up scheduled refresh",
              "completed": false
            },
            {
              "id": "task-pbi-8",
              "title": "Implement Row-Level Security for multi-tenant reporting",
              "completed": false
            }
          ],
          "attachments": [],
          "why": "Power BI is mentioned in almost every job posting. Data engineers increasingly need to bridge the gap between data pipelines and business insights.",
          "how": "Master Power BI Desktop first. Build dashboards connected to your data warehouse. Learn DAX deeply - it's the key to advanced analytics.",
          "order": 1
        },
        {
          "id": "ssis-etl",
          "title": "SSIS & Microsoft ETL Stack",
          "topics": [
            {
              "id": "topic-ssis-1",
              "title": "SSIS Architecture: Control Flow, Data Flow, Event Handlers",
              "completed": false
            },
            {
              "id": "topic-ssis-2",
              "title": "Data Flow Components: Sources, Transformations, Destinations",
              "completed": false
            },
            {
              "id": "topic-ssis-3",
              "title": "SSIS Transformations: Lookup, Merge Join, Conditional Split, Derived Column",
              "completed": false
            },
            {
              "id": "topic-ssis-4",
              "title": "Package Configuration: Variables, Parameters, Expressions",
              "completed": false
            },
            {
              "id": "topic-ssis-5",
              "title": "Error Handling: Logging, Event Handlers, Checkpoints",
              "completed": false
            },
            {
              "id": "topic-ssis-6",
              "title": "SSIS Catalog: Deployment, Execution, Monitoring",
              "completed": false
            },
            {
              "id": "topic-ssis-7",
              "title": "Performance Tuning: Buffer Sizing, Parallel Execution, Data Types",
              "completed": false
            },
            {
              "id": "topic-ssis-8",
              "title": "Modern Migration: SSIS to Azure Data Factory",
              "completed": false
            }
          ],
          "tasks": [
            {
              "id": "task-ssis-1",
              "title": "Install SQL Server Data Tools (SSDT) and create first SSIS project",
              "completed": false
            },
            {
              "id": "task-ssis-2",
              "title": "Build ETL package: Extract from CSV, Transform, Load to SQL Server",
              "completed": false
            },
            {
              "id": "task-ssis-3",
              "title": "Implement SCD Type 2 using SSIS Slowly Changing Dimension transform",
              "completed": false
            },
            {
              "id": "task-ssis-4",
              "title": "Create package with proper error handling and logging",
              "completed": false
            },
            {
              "id": "task-ssis-5",
              "title": "Deploy package to SSIS Catalog and schedule execution",
              "completed": false
            },
            {
              "id": "task-ssis-6",
              "title": "Migrate SSIS package to Azure Data Factory using IR",
              "completed": false
            }
          ],
          "attachments": [],
          "why": "SSIS remains widely used in enterprise environments. Understanding it shows versatility and helps with migration projects to modern cloud ETL.",
          "how": "Install SSDT. Build classic ETL packages for data warehouse loading. Practice migrating SSIS logic to Azure Data Factory.",
          "order": 2
        },
        {
          "id": "analytics-engineering",
          "title": "Analytics Engineering & Business Metrics",
          "topics": [
            {
              "id": "topic-ae-1",
              "title": "Analytics Engineering Role: Bridging Data Engineering and Analytics",
              "completed": false
            },
            {
              "id": "topic-ae-2",
              "title": "Semantic Layer: Metrics, Dimensions, Business Logic",
              "completed": false
            },
            {
              "id": "topic-ae-3",
              "title": "dbt Metrics: Defining Reusable Business Metrics",
              "completed": false
            },
            {
              "id": "topic-ae-4",
              "title": "Data Storytelling: Translating Data into Business Insights",
              "completed": false
            },
            {
              "id": "topic-ae-5",
              "title": "Understanding Business Domains: Finance, Marketing, Sales KPIs",
              "completed": false
            },
            {
              "id": "topic-ae-6",
              "title": "Stakeholder Communication: Requirements Gathering, Documentation",
              "completed": false
            },
            {
              "id": "topic-ae-7",
              "title": "Self-Service Analytics: Enabling Business Users",
              "completed": false
            }
          ],
          "tasks": [
            {
              "id": "task-ae-1",
              "title": "Define key business metrics for a sample e-commerce dataset",
              "completed": false
            },
            {
              "id": "task-ae-2",
              "title": "Create semantic layer with dbt exposures and metrics",
              "completed": false
            },
            {
              "id": "task-ae-3",
              "title": "Build executive dashboard answering key business questions",
              "completed": false
            },
            {
              "id": "task-ae-4",
              "title": "Document data models for business stakeholders",
              "completed": false
            },
            {
              "id": "task-ae-5",
              "title": "Present data insights to non-technical audience",
              "completed": false
            }
          ],
          "attachments": [],
          "why": "Job postings emphasize understanding business needs and collaborating with stakeholders. Analytics engineering bridges the gap between data and business value.",
          "how": "Learn to think like a business analyst. Understand common KPIs. Practice translating business requirements into data solutions.",
          "order": 3
        }
      ]
    },
    {
      "id": "phase-5",
      "title": "ML Engineering & GenAI",
      "duration": "1-2 months",
      "description": "Understand ML workflows, support ML systems in production, and leverage Generative AI for data engineering tasks. AI skills are increasingly mentioned in job postings.",
      "order": 5,
      "sections": [
        {
          "id": "ml-fundamentals",
          "title": "ML Fundamentals for Data Engineers",
          "topics": [
            {
              "id": "topic-ml-1",
              "title": "ML Workflow: Data Collection → Feature Engineering → Training → Evaluation → Deployment",
              "completed": false
            },
            {
              "id": "topic-ml-2",
              "title": "Feature Engineering: Encoding, Scaling, Imputation, Feature Crosses",
              "completed": false
            },
            {
              "id": "topic-ml-3",
              "title": "Train/Validation/Test Splits: Avoiding Data Leakage",
              "completed": false
            },
            {
              "id": "topic-ml-4",
              "title": "Model Evaluation: Accuracy, Precision, Recall, AUC-ROC",
              "completed": false
            },
            {
              "id": "topic-ml-5",
              "title": "Common ML Pitfalls: Leakage, Overfitting, Selection Bias",
              "completed": false
            },
            {
              "id": "topic-ml-6",
              "title": "Data Requirements: Volume, Velocity, Variety, Veracity",
              "completed": false
            },
            {
              "id": "topic-ml-7",
              "title": "Working with Data Scientists: Communication and Collaboration",
              "completed": false
            },
            {
              "id": "topic-ml-8",
              "title": "ML Data Pipelines: Training vs Inference Data",
              "completed": false
            }
          ],
          "tasks": [
            {
              "id": "task-ml-1",
              "title": "Complete Kaggle's Intro to Machine Learning course",
              "completed": false
            },
            {
              "id": "task-ml-2",
              "title": "Build end-to-end ML pipeline: data prep → training → evaluation",
              "completed": false
            },
            {
              "id": "task-ml-3",
              "title": "Create feature engineering pipeline with scikit-learn",
              "completed": false
            },
            {
              "id": "task-ml-4",
              "title": "Identify and fix data leakage in a sample dataset",
              "completed": false
            },
            {
              "id": "task-ml-5",
              "title": "Document data requirements for a predictive model",
              "completed": false
            }
          ],
          "attachments": [],
          "why": "Data engineers are increasingly asked to support ML workflows. Understanding the basics helps you build better pipelines for ML use cases and collaborate effectively.",
          "how": "Build a simple ML project end-to-end. Focus on data preparation, feature engineering, and understanding what data scientists need from you.",
          "order": 1
        },
        {
          "id": "genai-data-engineering",
          "title": "Generative AI for Data Engineering",
          "topics": [
            {
              "id": "topic-genai-1",
              "title": "LLM Fundamentals: Transformers, Tokens, Context Windows, Embeddings",
              "completed": false
            },
            {
              "id": "topic-genai-2",
              "title": "Azure OpenAI: GPT Models, Deployment, API Usage",
              "completed": false
            },
            {
              "id": "topic-genai-3",
              "title": "Copilot for Data: GitHub Copilot, Azure Copilot, Fabric Copilot",
              "completed": false
            },
            {
              "id": "topic-genai-4",
              "title": "RAG (Retrieval-Augmented Generation): Vector Stores, Embeddings, Chunking",
              "completed": false
            },
            {
              "id": "topic-genai-5",
              "title": "Data Quality with AI: Anomaly Detection, Data Validation, Enrichment",
              "completed": false
            },
            {
              "id": "topic-genai-6",
              "title": "Automating Data Tasks: Code Generation, Documentation, SQL Writing",
              "completed": false
            },
            {
              "id": "topic-genai-7",
              "title": "LLM Data Pipelines: Text Processing, Summarization, Classification",
              "completed": false
            },
            {
              "id": "topic-genai-8",
              "title": "Vector Databases: Pinecone, Weaviate, Azure AI Search",
              "completed": false
            }
          ],
          "tasks": [
            {
              "id": "task-genai-1",
              "title": "Set up Azure OpenAI and make first API call",
              "completed": false
            },
            {
              "id": "task-genai-2",
              "title": "Build data pipeline that uses LLM for text classification",
              "completed": false
            },
            {
              "id": "task-genai-3",
              "title": "Create RAG system for querying documentation",
              "completed": false
            },
            {
              "id": "task-genai-4",
              "title": "Use Copilot to accelerate SQL and Python development",
              "completed": false
            },
            {
              "id": "task-genai-5",
              "title": "Build data enrichment pipeline using LLM for entity extraction",
              "completed": false
            },
            {
              "id": "task-genai-6",
              "title": "Implement vector search for semantic data discovery",
              "completed": false
            }
          ],
          "attachments": [],
          "why": "Job postings increasingly mention GenAI, Azure OpenAI, and Copilot. Understanding how to leverage AI for data tasks is becoming a differentiator.",
          "how": "Start with Azure OpenAI. Build practical applications that enhance data pipelines. Focus on RAG for enterprise knowledge management.",
          "order": 2
        },
        {
          "id": "feature-stores",
          "title": "Feature Stores & MLOps",
          "topics": [
            {
              "id": "topic-fs-1",
              "title": "Feature Store Concepts: Feature Reuse, Consistency, Discovery",
              "completed": false
            },
            {
              "id": "topic-fs-2",
              "title": "Feast Architecture: Feature Repository, Registry, Materialization",
              "completed": false
            },
            {
              "id": "topic-fs-3",
              "title": "Online vs Offline Stores: Redis vs Parquet",
              "completed": false
            },
            {
              "id": "topic-fs-4",
              "title": "Feature Pipelines: Batch and Streaming Features",
              "completed": false
            },
            {
              "id": "topic-fs-5",
              "title": "Point-in-Time Joins: Avoiding Future Data Leakage",
              "completed": false
            },
            {
              "id": "topic-fs-6",
              "title": "MLflow: Experiment Tracking, Model Registry, Deployment",
              "completed": false
            },
            {
              "id": "topic-fs-7",
              "title": "Model Serving: Batch vs Real-time Inference",
              "completed": false
            },
            {
              "id": "topic-fs-8",
              "title": "ML Monitoring: Data Drift, Model Drift, Performance Degradation",
              "completed": false
            }
          ],
          "tasks": [
            {
              "id": "task-fs-1",
              "title": "Install Feast and create first feature repository",
              "completed": false
            },
            {
              "id": "task-fs-2",
              "title": "Define feature views for user and product features",
              "completed": false
            },
            {
              "id": "task-fs-3",
              "title": "Materialize features to online store (Redis)",
              "completed": false
            },
            {
              "id": "task-fs-4",
              "title": "Build training dataset with point-in-time correct features",
              "completed": false
            },
            {
              "id": "task-fs-5",
              "title": "Set up MLflow for experiment tracking",
              "completed": false
            },
            {
              "id": "task-fs-6",
              "title": "Log model parameters, metrics, and artifacts to MLflow",
              "completed": false
            },
            {
              "id": "task-fs-7",
              "title": "Deploy model from MLflow registry as REST API",
              "completed": false
            }
          ],
          "attachments": [],
          "why": "Feature stores are becoming standard ML infrastructure. Data engineers often own this layer, making it a valuable skill for roles at ML-heavy companies.",
          "how": "Set up Feast locally. Build a feature pipeline that computes features from raw data and serves them for both training and inference.",
          "order": 3
        }
      ]
    },
    {
      "id": "phase-6",
      "title": "Scale & Production Excellence",
      "duration": "Ongoing",
      "description": "Level up to senior engineer territory. Focus on building reliable, scalable systems, data governance, and leading technical decisions.",
      "order": 6,
      "sections": [
        {
          "id": "data-governance",
          "title": "Data Governance & Security",
          "topics": [
            {
              "id": "topic-gov-1",
              "title": "Data Governance Framework: Policies, Standards, Stewardship",
              "completed": false
            },
            {
              "id": "topic-gov-2",
              "title": "Data Quality: DQ Dimensions, Monitoring, Remediation",
              "completed": false
            },
            {
              "id": "topic-gov-3",
              "title": "Data Lineage: OpenLineage, Marquez, dbt Lineage, Purview",
              "completed": false
            },
            {
              "id": "topic-gov-4",
              "title": "Data Catalogs: DataHub, Amundsen, OpenMetadata, Microsoft Purview",
              "completed": false
            },
            {
              "id": "topic-gov-5",
              "title": "PII Protection: Data Masking, Tokenization, Encryption",
              "completed": false
            },
            {
              "id": "topic-gov-6",
              "title": "Privacy Compliance: GDPR, Data Retention, Right to be Forgotten",
              "completed": false
            },
            {
              "id": "topic-gov-7",
              "title": "Access Control: RBAC, Column-Level Security, Row-Level Security",
              "completed": false
            },
            {
              "id": "topic-gov-8",
              "title": "Data Contracts: Schema Definitions, SLAs, Breaking Change Management",
              "completed": false
            },
            {
              "id": "topic-gov-9",
              "title": "Audit and Compliance: Logging, Monitoring, Reporting",
              "completed": false
            }
          ],
          "tasks": [
            {
              "id": "task-gov-1",
              "title": "Set up DataHub or OpenMetadata for metadata management",
              "completed": false
            },
            {
              "id": "task-gov-2",
              "title": "Implement data lineage tracking in your pipelines",
              "completed": false
            },
            {
              "id": "task-gov-3",
              "title": "Create data quality checks with automated alerting",
              "completed": false
            },
            {
              "id": "task-gov-4",
              "title": "Implement PII detection and masking pipeline",
              "completed": false
            },
            {
              "id": "task-gov-5",
              "title": "Design RBAC model for multi-tenant data platform",
              "completed": false
            },
            {
              "id": "task-gov-6",
              "title": "Create data contract documentation for key datasets",
              "completed": false
            }
          ],
          "attachments": [],
          "why": "Data governance, security, and compliance are mentioned in every senior job posting. Companies need engineers who ensure data is secure, compliant, and trustworthy.",
          "how": "Set up a data catalog. Implement lineage tracking. Design access controls and PII handling for your data platform.",
          "order": 1
        },
        {
          "id": "infrastructure-as-code",
          "title": "Infrastructure as Code & DevOps",
          "topics": [
            {
              "id": "topic-iac-1",
              "title": "Terraform Fundamentals: Providers, Resources, State",
              "completed": false
            },
            {
              "id": "topic-iac-2",
              "title": "Terraform Modules: Reusable Infrastructure Components",
              "completed": false
            },
            {
              "id": "topic-iac-3",
              "title": "State Management: Remote State, Locking, Workspaces",
              "completed": false
            },
            {
              "id": "topic-iac-4",
              "title": "Docker: Dockerfile, Multi-stage Builds, Docker Compose",
              "completed": false
            },
            {
              "id": "topic-iac-5",
              "title": "Container Orchestration: Kubernetes Core Concepts",
              "completed": false
            },
            {
              "id": "topic-iac-6",
              "title": "K8s for Data: Spark on K8s, Airflow on K8s",
              "completed": false
            },
            {
              "id": "topic-iac-7",
              "title": "CI/CD: GitHub Actions, GitLab CI, Jenkins",
              "completed": false
            },
            {
              "id": "topic-iac-8",
              "title": "GitOps: ArgoCD, Flux, Infrastructure as Code Pipeline",
              "completed": false
            },
            {
              "id": "topic-iac-9",
              "title": "Secrets Management: HashiCorp Vault, AWS Secrets Manager",
              "completed": false
            }
          ],
          "tasks": [
            {
              "id": "task-iac-1",
              "title": "Create Terraform project for AWS S3 + IAM setup",
              "completed": false
            },
            {
              "id": "task-iac-2",
              "title": "Build reusable Terraform module for data lake resources",
              "completed": false
            },
            {
              "id": "task-iac-3",
              "title": "Set up remote state in S3 with DynamoDB locking",
              "completed": false
            },
            {
              "id": "task-iac-4",
              "title": "Containerize ETL pipeline with Docker",
              "completed": false
            },
            {
              "id": "task-iac-5",
              "title": "Deploy containerized app to local Kubernetes (minikube)",
              "completed": false
            },
            {
              "id": "task-iac-6",
              "title": "Create CI/CD pipeline that deploys infrastructure changes",
              "completed": false
            },
            {
              "id": "task-iac-7",
              "title": "Set up GitHub Actions for dbt project testing and deployment",
              "completed": false
            }
          ],
          "attachments": [],
          "why": "Senior data engineers don't click around in consoles - they codify infrastructure. Docker, K8s, Terraform, and CI/CD are mentioned in every senior role.",
          "how": "Rewrite your cloud setup using Terraform. Containerize your pipelines with Docker. Set up a CI/CD pipeline with GitHub Actions or Azure DevOps.",
          "order": 2
        },
        {
          "id": "data-platform-design",
          "title": "Data Platform Design",
          "topics": [
            {
              "id": "topic-platform-1",
              "title": "Data Mesh: Domain Ownership, Data Products, Federated Governance",
              "completed": false
            },
            {
              "id": "topic-platform-2",
              "title": "Data Lakehouse: Combining Lake Flexibility with Warehouse Features",
              "completed": false
            },
            {
              "id": "topic-platform-3",
              "title": "Medallion Architecture: Bronze, Silver, Gold Layers",
              "completed": false
            },
            {
              "id": "topic-platform-4",
              "title": "Metadata Management: Technical and Business Metadata",
              "completed": false
            },
            {
              "id": "topic-platform-5",
              "title": "Data Catalogs: DataHub, Amundsen, OpenMetadata",
              "completed": false
            },
            {
              "id": "topic-platform-6",
              "title": "Data Governance: Access Control, PII, GDPR Compliance",
              "completed": false
            },
            {
              "id": "topic-platform-7",
              "title": "Cost Optimization: Right-sizing, Reserved Capacity, Auto-scaling",
              "completed": false
            },
            {
              "id": "topic-platform-8",
              "title": "Multi-tenant Architecture: Isolation, Resource Allocation",
              "completed": false
            }
          ],
          "tasks": [
            {
              "id": "task-platform-1",
              "title": "Design data platform architecture for fictional company",
              "completed": false
            },
            {
              "id": "task-platform-2",
              "title": "Create architecture diagram using draw.io or Lucidchart",
              "completed": false
            },
            {
              "id": "task-platform-3",
              "title": "Document ADRs (Architecture Decision Records) for key choices",
              "completed": false
            },
            {
              "id": "task-platform-4",
              "title": "Set up DataHub or OpenMetadata for metadata management",
              "completed": false
            },
            {
              "id": "task-platform-5",
              "title": "Implement medallion architecture in Delta Lake",
              "completed": false
            },
            {
              "id": "task-platform-6",
              "title": "Create cost estimation and optimization plan",
              "completed": false
            }
          ],
          "attachments": [],
          "why": "Understanding architecture patterns helps you make better decisions and have meaningful input on platform design. Data Mesh is increasingly mentioned in job postings.",
          "how": "Design a data platform for a fictional company. Document architecture decisions, create diagrams, and think through scalability and cost implications.",
          "order": 3
        },
        {
          "id": "reliability-observability",
          "title": "Reliability & Observability",
          "topics": [
            {
              "id": "topic-rel-1",
              "title": "SLIs, SLOs, SLAs: Defining Data Quality Metrics",
              "completed": false
            },
            {
              "id": "topic-rel-2",
              "title": "Pipeline Monitoring: Latency, Throughput, Error Rates",
              "completed": false
            },
            {
              "id": "topic-rel-3",
              "title": "Logging: Structured Logs, Log Aggregation, ELK Stack",
              "completed": false
            },
            {
              "id": "topic-rel-4",
              "title": "Metrics: Prometheus, Grafana, Custom Metrics",
              "completed": false
            },
            {
              "id": "topic-rel-5",
              "title": "Alerting: PagerDuty, OpsGenie, Alert Fatigue",
              "completed": false
            },
            {
              "id": "topic-rel-6",
              "title": "Data Lineage: OpenLineage, Marquez, dbt Lineage",
              "completed": false
            },
            {
              "id": "topic-rel-7",
              "title": "Incident Management: Runbooks, Post-mortems, On-call",
              "completed": false
            },
            {
              "id": "topic-rel-8",
              "title": "Chaos Engineering: Testing Failure Scenarios",
              "completed": false
            }
          ],
          "tasks": [
            {
              "id": "task-rel-1",
              "title": "Define SLOs for data freshness and quality",
              "completed": false
            },
            {
              "id": "task-rel-2",
              "title": "Set up Grafana dashboard for pipeline monitoring",
              "completed": false
            },
            {
              "id": "task-rel-3",
              "title": "Implement structured logging in ETL pipeline",
              "completed": false
            },
            {
              "id": "task-rel-4",
              "title": "Create Prometheus metrics for custom pipeline stats",
              "completed": false
            },
            {
              "id": "task-rel-5",
              "title": "Set up alerting rules for pipeline failures",
              "completed": false
            },
            {
              "id": "task-rel-6",
              "title": "Write runbook for common pipeline failure scenarios",
              "completed": false
            },
            {
              "id": "task-rel-7",
              "title": "Conduct post-mortem for a simulated incident",
              "completed": false
            }
          ],
          "attachments": [],
          "why": "Production systems fail. How you handle failures, monitor systems, and maintain reliability separates good engineers from great ones.",
          "how": "Add comprehensive monitoring to your pipelines. Set up dashboards, alerts, and runbooks. Practice incident response scenarios.",
          "order": 4
        },
        {
          "id": "leadership-communication",
          "title": "Technical Leadership & Communication",
          "topics": [
            {
              "id": "topic-lead-1",
              "title": "RFC Writing: Problem Statement, Proposed Solution, Alternatives",
              "completed": false
            },
            {
              "id": "topic-lead-2",
              "title": "Technical Presentations: Storytelling, Visuals, Audience Awareness",
              "completed": false
            },
            {
              "id": "topic-lead-3",
              "title": "Stakeholder Management: Understanding Business Needs",
              "completed": false
            },
            {
              "id": "topic-lead-4",
              "title": "Mentoring: Code Reviews, Pair Programming, Feedback",
              "completed": false
            },
            {
              "id": "topic-lead-5",
              "title": "Cross-functional Collaboration: Working with DS, Analysts, Product",
              "completed": false
            },
            {
              "id": "topic-lead-6",
              "title": "Trade-off Analysis: Cost vs Performance vs Complexity",
              "completed": false
            },
            {
              "id": "topic-lead-7",
              "title": "Building Team Culture: Documentation, Best Practices, Standards",
              "completed": false
            },
            {
              "id": "topic-lead-8",
              "title": "Career Growth: Building Your Brand, Networking, Public Speaking",
              "completed": false
            }
          ],
          "tasks": [
            {
              "id": "task-lead-1",
              "title": "Write RFC for a data platform improvement",
              "completed": false
            },
            {
              "id": "task-lead-2",
              "title": "Present technical concept to non-technical audience",
              "completed": false
            },
            {
              "id": "task-lead-3",
              "title": "Mentor someone junior: pair on a coding problem",
              "completed": false
            },
            {
              "id": "task-lead-4",
              "title": "Create documentation standards for team projects",
              "completed": false
            },
            {
              "id": "task-lead-5",
              "title": "Write blog post about a DE topic you learned",
              "completed": false
            },
            {
              "id": "task-lead-6",
              "title": "Give a tech talk or presentation at a meetup",
              "completed": false
            }
          ],
          "attachments": [],
          "why": "Senior roles require more than technical skills. Job postings emphasize stakeholder communication, collaboration with business teams, and documentation skills.",
          "how": "Write an RFC for a project you're working on. Practice explaining technical concepts to non-technical stakeholders. Mentor someone junior.",
          "order": 5
        }
      ]
    }
  ],
  "lastUpdated": "2025-12-23T10:00:00.000Z"
}